{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", sep=\";\")\n",
    "descriptions = df[\"descricao\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese') # portuguese stopwords\n",
    "count_vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "remove_stopwords = count_vectorizer.build_analyzer() # function to remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def clean_text(descriptions: list[str]) -> list[str]:\n",
    "    \"\"\"Return the same input list of strings with a clean text.\"\"\"\n",
    "    descriptions_clean = []\n",
    "    for d in descriptions:\n",
    "        d = unidecode(d.lower())                           # remove accents and put letters in lower case\n",
    "        d = re.sub(r'[^a-z ]', '', d)                     # remove all that is not a letter or space\n",
    "        d = re.sub(r'\\b[a-z]{1,2}\\b', '', d)                     # remove all single or pair of letters\n",
    "        d = ' '.join([word for word in remove_stopwords(d)])    # remove stopwords / remove double space / remove space at start and end of descriptions\n",
    "                    # 'count_vectorizer.build_analyzer()'/'remove_stopwords' return the words separates in list format, so was used the ' '.join()\n",
    "        descriptions_clean.append(d)\n",
    "    return descriptions_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Description before: {descriptions[0]}\")\n",
    "descriptions_clean = clean_text(descriptions)\n",
    "print(f\"Description after: {descriptions_clean[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for d in descriptions_clean:\n",
    "    w = d.split(\" \")\n",
    "    for i in w:\n",
    "        words.append(i)\n",
    "print(words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, c = np.unique(words, return_counts=True)\n",
    "df_words = pd.DataFrame({\n",
    "    \"words\": w,\n",
    "    \"count\": c\n",
    "})\n",
    "df_words = df_words.sort_values(by=[\"count\"], ascending=False) # order by count desc\n",
    "df_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for w in df_words[\"words\"].values:\n",
    "    if w != df_words[\"words\"].values[-1]:\n",
    "        text += w + \" \"\n",
    "    else: \n",
    "        text += w\n",
    "\n",
    "with open(\"words.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
